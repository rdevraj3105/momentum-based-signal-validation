{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678a3fee-dedf-457c-959e-b62d07390805",
   "metadata": {},
   "source": [
    "# Momentum-Based Signal Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "718de208-d7b1-4391-911b-30e18bdd6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "562f1409-3cc5-42d3-9067-93ea49efd3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import jarque_bera, shapiro, normaltest\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "095233dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wb/jhyg6q791mj15523rmpw4r1m0000gp/T/ipykernel_914/3367302652.py:1: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(\"AMZN AAPL GOOG\",start=\"2020-01-01\", end=\"2023-01-01\").head()\n",
      "[*********************100%***********************]  3 of 3 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = yf.download(\"AMZN AAPL GOOG\",start=\"2020-01-01\", end=\"2023-01-01\").head()\n",
    "missing_values = data.isnull().sum().sum()\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d70b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c94c6ef5",
   "metadata": {},
   "source": [
    "*** Data Collection ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f50a1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stock_data(tickers, start_date = '2020-01-01', end_date = '2024-07-01'):\n",
    "    stock_data = {}\n",
    "    for ticker in tickers:\n",
    "        data = yf.download(ticker, start= start_date, end = end_date, auto_adjust=False)\n",
    "\n",
    "        if len(data) > 0:\n",
    "            stock_data[ticker] = data\n",
    "        else:\n",
    "            print(\"No data found\")\n",
    "\n",
    "    return stock_data\n",
    "\n",
    "def quality_check(data, ticker):\n",
    "    # Check quality of data for one stock\n",
    "    # data = data.xs(ticker, axis=1, level=1)\n",
    "    total_days = len(data)\n",
    "    missing_values = data.isnull().sum().sum()\n",
    "\n",
    "    zero_volume_days = (data['Volume'] == 0).sum()\n",
    "\n",
    "    daily_returns = data['Close'].pct_change()\n",
    "    extreme_moves = (abs(daily_returns) > 0.2).sum()\n",
    "\n",
    "    print(f\"{ticker}:\")\n",
    "    print(f\"Total observations: {total_days}\")\n",
    "    print(f\"Missing values: {missing_values}\")\n",
    "    print(f\"Zero volume days: {zero_volume_days}\")\n",
    "    print(f\"Extreme moves (>20%): {extreme_moves}\")\n",
    "\n",
    "    return {\n",
    "        'total_days': total_days,\n",
    "        'missing_values': missing_values,\n",
    "        'zero_volume': zero_volume_days,\n",
    "        'extreme_moves': extreme_moves\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_returns_stats(data, ticker):\n",
    "    print(\"Return Statistics for \" + ticker)\n",
    "    returns = data['Close'].pct_change().dropna()\n",
    "\n",
    "    # basic stats\n",
    "\n",
    "    mean_return = float(returns.mean())\n",
    "    volatility = float(returns.std())\n",
    "    annual_return = mean_return * 252\n",
    "    annual_vol = volatility * np.sqrt(252)\n",
    "\n",
    "\n",
    "    print(f\"Daily return: {mean_return:.4f} ({annual_return:.2%} annualized)\")\n",
    "    print(f\"Daily volatility: {volatility:.4f} ({annual_vol:.2%} annualized)\")\n",
    "    print(f\"Skewness: {float(returns.skew()):.3f}\")\n",
    "    print(f\"Kurtosis: {float(returns.kurtosis()):.3f}\")\n",
    "\n",
    "    \n",
    "    #Test for normality\n",
    "    jb_stat, jb_pvalue = jarque_bera(returns)\n",
    "\n",
    "    print(f\"*** Normality Test ***\")\n",
    "    print(f\"Jarque Bera p-value: {jb_pvalue:.6f}\")\n",
    "    if jb_pvalue >= 0.05:\n",
    "        print(\"Returns are normal\")\n",
    "    else:\n",
    "        print('Returns are NOT normal')\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87134a11",
   "metadata": {},
   "source": [
    "*** Feature Engineering, Creating Momentum Signals ***\n",
    "- Main Idea: stocks that have been going up, might continue going up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "268d66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_momentum_features(data):\n",
    "    df = data.copy()\n",
    "    #one day returns\n",
    "    df['returns_1d'] = df['Close'].pct_change()\n",
    "\n",
    "    #momentum periods of one week, 2 weeks, 1 month, 3 months\n",
    "    momentum_periods = [5, 10, 20, 60]\n",
    "\n",
    "    for period in momentum_periods:\n",
    "        df[f'momentum_{period}d'] = df['Close'].pct_change(period)\n",
    "\n",
    "    # moving averages\n",
    "    ma_periods = [10, 20, 50]\n",
    "    for period in ma_periods:\n",
    "        df[f'sma_{period}d'] = df['Close'].pct_change(period).mean()\n",
    "        df[f'above_sma_{period}'] =  (df['Close'] > df[f'sma_{period}']).astype(int)\n",
    "        print(period + \"--day moving average signal\")\n",
    "\n",
    "    # volatility\n",
    "\n",
    "    df['vol_10d'] = df['returns_1d'].rolling(10).std()\n",
    "    df['vol_20d'] = df['returns_1d'].rolling(20).std()\n",
    "    print(\"***Volatility features***\")\n",
    "\n",
    "    # RSI - Relative Strength Index - momentum oscillator\n",
    "    def calculate_rsi(prices, window = 14):\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window = window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window = window).mean()\n",
    "        rs = gain/loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "\n",
    "    df['rsi'] = calculate_rsi(df['Close'])\n",
    "    df['rsi_oversold'] = calculate_rsi(df['Close'] < 30).astype(int)\n",
    "    df['rsi_undersold'] = calculate_rsi(df['close'] > 70).astype(int)\n",
    "\n",
    "    print(\"*** RSI momentum indicator ***\")\n",
    "\n",
    "    df['vol_sma_20'] = df['Volume'].rolling(20).mean()\n",
    "    df['volume_ratio'] = df['Volume'] / df['volume_sma_20']\n",
    "    df['high_volume'] = (df['volume_ratio'] > 1.5).astype(int)\n",
    "    print(\"*** Volume Features ***\")\n",
    "\n",
    "    df['high_20d'] = df['High'].rolling(20).max()\n",
    "    df['low_20d'] = df['Low'].rolling(20).min()\n",
    "    df['price_position'] = (df['Close'] - df['low_20d']) / (df['high_20d'] - df['low_20d'])\n",
    "    print(\"*** Price Position Feature ***\")\n",
    "    \n",
    "    print(f\"\\n There are {len([col for col in df.columns if col not in data.columns])} new features created\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b0da9d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_variable(data, forward_days = 5):\n",
    "    print(\"Creating target: {forward_days}-day forward return\")\n",
    "    target = data['Close'].pct_change(forward_days).shift(-forward_days)\n",
    "\n",
    "    print(f\"Target mean: {target.mean():.4f}\")\n",
    "    print(f\"Target std: {target.std():.4f}\")\n",
    "    print(f\"Valid predictions: {target.count()}\\n\")\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f301a5d",
   "metadata": {},
   "source": [
    "*** ML - Test if signals work ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ce2719b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_ml_dataset(feature_data, target, train_ratio = 0.7):\n",
    "    og_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "    feature_columns = [col for col in feature_data.columns if col not in og_columns]\n",
    "\n",
    "    X = feature_data[feature_columns].copy()\n",
    "    y = target.copy()\n",
    "\n",
    "    ml_data = pd.concat([X, y.rename('target')], axis = 1).dropna()\n",
    "\n",
    "    print(f\"Original data points: {len(feature_data)}\")\n",
    "    print(f\"Features: {len(feature_columns)}\")\n",
    "    print(f\"First five feature names: {feature_columns[:5]}\")\n",
    "\n",
    "    if len(ml_data) < 100:\n",
    "        print(f\"Not enough data for ML: ({len(ml_data)} < 100)\")\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    X_clean = ml_data[feature_columns]\n",
    "    y_clean = ml_data['target']\n",
    "\n",
    "    # Time series split!!!!\n",
    "    split = int(len(ml_data) * train_ratio)\n",
    "\n",
    "    X_train = X_clean.iloc[:split]\n",
    "    X_test = X_clean.iloc[split:]\n",
    "    y_train = y_clean.iloc[:split]\n",
    "    y_test = y_clean.iloc[split:]\n",
    "\n",
    "    print(f\"Training set has {len(X_train)} observations\")\n",
    "    print(f\"Testing set has {len(X_test)} observations\\n\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, feature_columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5563b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_models(X_train, X_test, y_train, y_test, feature_names):\n",
    "    print(\"Training and testing models...\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    models = {\n",
    "        'Linear Regression' : LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(n_estimator = 100, max_depth = 5, random_state = 42)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model  in models.items():\n",
    "        if name == 'Linear Regression':\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            train_pred = model.predict(X_train_scaled)\n",
    "            test_pred = model.predict(X_test_scaled)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            train_pred = model.predict(X_train)\n",
    "            test_pred = model.predict(X_test)\n",
    "    \n",
    "        train_r2 = r2_score(y_train, train_pred)\n",
    "        test_r2 = r2_score(y_test, test_pred)\n",
    "\n",
    "        results[name] = {\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2' : test_r2,\n",
    "            'overfitting': train_r2 - test_r2\n",
    "        }\n",
    "\n",
    "        print(f\"Train R^2: {train_r2:.4f}\")\n",
    "        print(f\"Test R^2: {test_r2:.4f}\")\n",
    "        print(f\"Overfitting: {train_r2 - test_r2:.4}\")\n",
    "\n",
    "        #random forest feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature' : feature_names,\n",
    "                'importance': model.feature_importances_\n",
    "            })\n",
    "            importance_df.sort_values('importance', ascending = False)\n",
    "            print(f\"Top 5 important features:\")\n",
    "            for _, row in importance_df.head().iterrows():\n",
    "                print(f\"- {row['feature']}: {row['importance']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3da76960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, y):\n",
    "    print(f\"Cross validation\")\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits = 3)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators = 50, max_depth = 3, random_state = 42)\n",
    "    cv_scores = cross_val_score(model, X, y, cv = tscv, scoring = 'r^2')\n",
    "    print(f\"CV results:\")\n",
    "    print(f\"- Mean R^2:{cv_scores.mean():.4f}\")\n",
    "    print(f\"- Std R^2: {cv_scores.std():.4f}\")\n",
    "    print(f\"- Individual Scores:{cv_scores.round(4)}\")\n",
    "\n",
    "    if cv_scores.mean() > 0.02:\n",
    "        print(f\"Model shows predictive power\")\n",
    "    else:\n",
    "        print(\"Weak predictive power\")\n",
    "\n",
    "    if cv_scores.std() < 0.05:\n",
    "        print(\"Stable performance across time periods\")\n",
    "    else:\n",
    "        print(\"Unstable - performance varies\")\n",
    "\n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "61127d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker            NVDA\n",
      "Date                  \n",
      "2020-01-02    5.971747\n",
      "2020-01-03    5.876164\n",
      "2020-01-06    5.900805\n",
      "2020-01-07    5.972246\n",
      "2020-01-08    5.983446\n",
      "...                ...\n",
      "2024-06-24  118.072693\n",
      "2024-06-25  126.050171\n",
      "2024-06-26  126.360077\n",
      "2024-06-27  123.950844\n",
      "2024-06-28  123.500984\n",
      "\n",
      "[1130 rows x 1 columns]\n",
      "NVDA:\n",
      "Total observations: 1130\n",
      "Missing values: 0\n",
      "Zero volume days: Ticker\n",
      "NVDA    0\n",
      "dtype: int64\n",
      "Extreme moves (>20%): Ticker\n",
      "NVDA    1\n",
      "dtype: int64\n",
      "Return Statistics for NVDA\n",
      "Daily return: 0.0033 (82.08% annualized)\n",
      "Daily volatility: 0.0341 (54.07% annualized)\n",
      "Skewness: 0.446\n",
      "Kurtosis: 4.267\n",
      "*** Normality Test ***\n",
      "Jarque Bera p-value: 0.000000\n",
      "Returns are NOT normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/wb/jhyg6q791mj15523rmpw4r1m0000gp/T/ipykernel_914/3220334234.py:44: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  mean_return = float(returns.mean())\n",
      "/var/folders/wb/jhyg6q791mj15523rmpw4r1m0000gp/T/ipykernel_914/3220334234.py:45: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  volatility = float(returns.std())\n",
      "/var/folders/wb/jhyg6q791mj15523rmpw4r1m0000gp/T/ipykernel_914/3220334234.py:52: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"Skewness: {float(returns.skew()):.3f}\")\n",
      "/var/folders/wb/jhyg6q791mj15523rmpw4r1m0000gp/T/ipykernel_914/3220334234.py:53: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  print(f\"Kurtosis: {float(returns.kurtosis()):.3f}\")\n"
     ]
    }
   ],
   "source": [
    "tickers = ['AAPL', 'TSLA', 'GOOG', 'NVDA', 'MSFT']\n",
    "\n",
    "stock_data = get_stock_data(tickers)\n",
    "\n",
    "\n",
    "main_ticker = list(stock_data.keys())[3]\n",
    "\n",
    "data = stock_data[main_ticker]\n",
    "print(data)\n",
    "\n",
    "quality_report = quality_check(data, main_ticker)\n",
    "    \n",
    "returns = analyze_returns_stats(data, main_ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8c864af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling [window=2,center=False,axis=0,method=single]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2995f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
